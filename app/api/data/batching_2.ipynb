{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1323\n"
     ]
    }
   ],
   "source": [
    "# Import raw comment data\n",
    "import json\n",
    "from id_hash import hash\n",
    "\n",
    "with open('data/physics_podcast_1.json', 'r') as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "messages = [c['message'] for c in corpus]\n",
    "for c in corpus:\n",
    "    c['id'] = hash(c['message'], c['message'], c['message'])\n",
    "    c['likes'] = 0\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': -400, 'message': 'Love your channel these days ... killing it with the guests ...', 'id': '1e28d7861f918e66', 'likes': 0, 'topics': [('these days', 3, 5, 'DATE'), ('channel', 2, 3, 'NOUN_CHUNK'), ('', 8, 8, 'NOUN_CHUNK'), ('guests', 10, 11, 'NOUN_CHUNK')], 'sentiment': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Analyze each comment\n",
    "import re\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "def clean(dirty):\n",
    "    clean = dirty.lower().strip()\n",
    "    clean = re.sub(\"(\\'s)\", \"\", clean)\n",
    "    clean = re.sub(r'<.*?( \\/)*>', '', clean)\n",
    "    return clean\n",
    "\n",
    "def analyze(m):\n",
    "    doc = nlp(m)\n",
    "    topics = []\n",
    "    ents = []\n",
    "    sentiment = doc._.polarity\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        ents.append(ent.text)\n",
    "        topics.append((clean(ent.text), ent.start, ent.end, ent.label_))\n",
    "        \n",
    "    for chunk in doc.noun_chunks:\n",
    "        start, end = chunk.start, chunk.end\n",
    "        for token in chunk:\n",
    "            if not token.is_stop:\n",
    "                break\n",
    "            start += 1 # Remove stop words from start\n",
    "        span = doc[start:end]\n",
    "        if (not span) | (clean(span.text) in ents):\n",
    "            pass; # Avoid duplicates\n",
    "        topics.append((clean(span.text), span.start, span.end, 'NOUN_CHUNK'))\n",
    "        \n",
    "    return topics, sentiment\n",
    "\n",
    "for c in corpus:\n",
    "    topics, sentiment = analyze(c['message'])\n",
    "    c['topics'] = topics\n",
    "    c['sentiment'] = round(sentiment, 3)\n",
    "    \n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('these days', 'DATE', [('1e28d7861f918e66', 0, 0.5), ('548aa7a35387fd32', 0, 0.25), ('79931d6d28e2b50b', 0, 0.375)]) ('channel', 'NOUN_CHUNK', [('1e28d7861f918e66', 0, 0.5), ('3ee7f3cf37c22024', 0, 0.0)]) 2299\n"
     ]
    }
   ],
   "source": [
    "# Cluster comment topics\n",
    "from collections import Counter\n",
    "\n",
    "def cluster_comments(comments, user_labs=[]): \n",
    "    clustered = {} # by token\n",
    "    for c in comments:\n",
    "        if 'topics' not in c.keys():\n",
    "            pass\n",
    "        for token, start, end, label in c['topics']:\n",
    "            edge = (c['id'], label, c['likes'], c['sentiment'])            \n",
    "            if token in clustered.keys():\n",
    "                clustered[token].append(edge)\n",
    "            else:\n",
    "                clustered[token] = [edge] # Sort by token\n",
    "\n",
    "    # Group and Count\n",
    "    named_entities = []\n",
    "    noun_chunks = []\n",
    "    frequency = {}\n",
    "    for token in clustered:# {...token: [...(id, label, likes, sentiment)]}\n",
    "        t = clustered[token] \n",
    "        n = len(t)\n",
    "        if label in user_labs:\n",
    "            label = user_labs[label]\n",
    "        else:\n",
    "            all_labels = [lab for (i, lab, lik, s) in t]\n",
    "            label = Counter(all_labels).most_common(1)[0][0]\n",
    "        edges = [(i, lik, s) for (i, lab, lik, s) in clustered[token]]\n",
    "        if label == 'NOUN_CHUNK':\n",
    "            noun_chunks.append((token, label, edges))\n",
    "            frequency[token] = len(edges)\n",
    "        else:\n",
    "            named_entities.append((token, label, edges))\n",
    "            frequency[token] = len(edges)\n",
    "    return named_entities, noun_chunks, frequency\n",
    "\n",
    "named_entities, noun_chunks, frequency = cluster_comments(corpus)\n",
    "\n",
    "print(named_entities[0], noun_chunks[0], len(frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('@church', 'ORG', 18, [('8ad2436dc945ea88', 0, 0.375), ('ad1a615ee1dc55bd', 0, 0.375), ('d425ff0c2880f7da', 0, 0.402), ('917cf84f93cad293', 0, 0.0), ('40d4379d6e02af0b', 0, 0.0), ('40d4379d6e02af0b', 0, 0.0), ('668d696dc728aebb', 0, 0.0), ('0b2db982638ae2f5', 0, -0.9), ('0b2db982638ae2f5', 0, -0.9), ('97bdaa337027d1c7', 0, 0.0), ('a5b7455a6685d7fe', 0, 0.0), ('fdc28f0a5fd1045d', 0, 0.0), ('3bce5158010f9b64', 0, 0.175), ('3fc86ff46d40c063', 0, 0.0), ('9d6ababab4d32e10', 0, 0.6), ('9d6ababab4d32e10', 0, 0.6), ('3b8419c73cbbfd9d', 0, -0.175), ('911733f598249d40', 0, 0.0)])\n"
     ]
    }
   ],
   "source": [
    "# Parse named entities\n",
    "def cluster_ne(named_entities, frequency, user_subs=[]):\n",
    "    subs = {}\n",
    "    subbed = {}\n",
    "    def add(token, label, edges):\n",
    "        if token in subbed.keys():\n",
    "            new_edges = subbed[token][1] + edges\n",
    "            subbed[token] = (label, new_edges)\n",
    "        else:\n",
    "            subbed[token] = (label, edges)\n",
    "\n",
    "    named_entity_longest = sorted(named_entities, key=lambda e: len(e[0]), reverse=True)\n",
    "    for token, label, edges in named_entity_longest:\n",
    "        used = False\n",
    "        if token in user_subs:\n",
    "            repl = user_subs[token]\n",
    "            subs[token] = repl  \n",
    "            add(repl, label, edges)\n",
    "            used = True\n",
    "        elif label == 'PERSON':\n",
    "            included = [(key, frequency[key]) for key in subbed.keys() if token in key]\n",
    "            if len(included) > 0:\n",
    "                (repl, freq) = sorted(included, key=lambda i: i[1], reverse=True)[0]\n",
    "                collision = freq / frequency[token]\n",
    "                if (freq > 2) & (collision > 0.3):\n",
    "                    subs[token] = repl if not repl in subs.keys() else subs[repl]\n",
    "                    add(subs[token], label, edges)\n",
    "                    used = True\n",
    "        elif label in ['CARDINAL', 'TIME', 'QUANTITY']:\n",
    "            used = True # Remove\n",
    "        if used == False:\n",
    "            add(token, label, edges)\n",
    "            \n",
    "    named_entities_subbed = []\n",
    "    for token in subbed:\n",
    "        (label, edges) = subbed[token]\n",
    "        named_entities_subbed.append((token, label, len(edges), edges))\n",
    "    named_entities_sorted = sorted(named_entities_subbed, key=lambda c: c[2], reverse=True)\n",
    "    return named_entities_sorted, subs\n",
    "    \n",
    "user_subs = {'eric': 'eric weinstein', 'max': 'max tegmark', 'gu': 'geometric unity'}\n",
    "named_entities_sorted, ne_subs = cluster_ne(named_entities, frequency, user_subs)\n",
    "print(named_entities_sorted[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('god', 'NOUN_CHUNK', 17, [('21bb304bbc32c13d', 0, 0.0), ('8b83ab32a47f7e76', 0, 0.0), ('061ba9aa8d19dabc', 0, 0.0), ('ac16cc7d0e8b5862', 0, -0.188), ('d98d6c33f78113dd', 0, 0.0), ('d2029b7113a115a9', 0, 0.0), ('c2a9f53fae851fe5', 0, 0.292), ('f2d72379797356a2', 0, 0.183), ('c3315c3e5a40ec3b', 0, 0.0), ('c3315c3e5a40ec3b', 0, 0.0), ('48f4894a154dff49', 0, 0.8), ('48f4894a154dff49', 0, 0.8), ('c01be916764d2444', 0, 0.2), ('bb50b1c468270f1a', 0, 0.0), ('a803ef2f327a5fb9', 0, -0.25), ('e2bd2aa941ddf209', 0, -0.3), ('49967926f90c4dd1', 0, -0.312)])\n"
     ]
    }
   ],
   "source": [
    "# Parse noun chunks\n",
    "import html\n",
    "from itertools import combinations\n",
    "\n",
    "def cluster_nc(noun_chunks, ne_tokens, frequency, user_subs=[]):\n",
    "    ngrams = {}\n",
    "    for edge in noun_chunks:\n",
    "        cleaned = edge[0]\n",
    "        cleaned = html.unescape(cleaned)\n",
    "\n",
    "        # Search all permutations of words\n",
    "        words = cleaned.split(\" \")\n",
    "        local_ngrams = []            \n",
    "        max_n = 4\n",
    "        for n in range(max(len(words), max_n)):\n",
    "            local_ngrams += [list(x) for x in combinations(words, n)]\n",
    "\n",
    "        used = False    \n",
    "        for n in local_ngrams:\n",
    "            cleaned = \" \".join(n).strip()\n",
    "            if (cleaned not in ne_tokens) & (cleaned not in user_subs) & (cleaned != \"\"):\n",
    "                if cleaned in ngrams:\n",
    "                    ngrams[cleaned].append(edge)\n",
    "                    used = True\n",
    "        if not used:\n",
    "            ngrams[cleaned] = [edge]            \n",
    "    ngrams_by_length = sorted([(n, len(ngrams[n]), ngrams[n]) for n in ngrams], key=lambda i : len(i[0]), reverse=True)\n",
    "\n",
    "    ng_clust = {}\n",
    "    ngram_subs = {}\n",
    "    for token, freq, edges in ngrams_by_length:\n",
    "        included = [(c, len(ngrams[c])) for c in ng_clust if token in c]\n",
    "\n",
    "        if len(included) == 0:\n",
    "            ng_clust[token] = ngrams[token]\n",
    "        else:\n",
    "            # most similar !== most frequent\n",
    "            most_similar = sorted(included, key=lambda i: i[1], reverse=True)[0]\n",
    "\n",
    "            if (most_similar[1] > 2) & (most_similar[1] > len(ngrams[token])*0.6):\n",
    "                ngram_subs[token] = most_similar[0]\n",
    "                ng_clust[most_similar[0]] += ngrams[token]\n",
    "            else:\n",
    "                ng_clust[token] = ngrams[token]\n",
    "\n",
    "    ng_topics = [(n, ng_clust[n][0][1], len(ng_clust[n][0][2]), ng_clust[n][0][2]) for n in ng_clust]\n",
    "    ng_topics_sorted = sorted(ng_topics, key=lambda t : t[2], reverse=True)\n",
    "    return ng_topics_sorted, ngram_subs\n",
    "                    \n",
    "ne_tokens = [token for (token, label, n, edges) in named_entities_sorted]\n",
    "noun_chunks_sorted, ngram_subs = cluster_nc(noun_chunks, ne_tokens, frequency, ne_subs)\n",
    "print(noun_chunks_sorted[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('eric weinstein', 'PERSON', 215.0, 0, {'pos': 95, 'neg': 21, 'neu': 99})\n",
      "('brian keating', 'PERSON', 123.0, 0, {'pos': 62, 'neg': 6, 'neu': 55})\n",
      "('max tegmark', 'PERSON', 101.0, 0, {'pos': 43, 'neg': 16, 'neu': 42})\n",
      "('physics', 'NOUN_CHUNK', 51.0, 0, {'pos': 21, 'neg': 5, 'neu': 25})\n",
      "('universe', 'NOUN_CHUNK', 39.0, 0, {'pos': 12, 'neg': 5, 'neu': 22})\n",
      "('geometric unity', 'PERSON', 30.0, 0, {'pos': 4, 'neg': 4, 'neu': 22})\n",
      "('entropy', 'NOUN_CHUNK', 29.0, 0, {'pos': 11, 'neg': 3, 'neu': 15})\n",
      "('time', 'NOUN_CHUNK', 29.0, 0, {'pos': 7, 'neg': 7, 'neu': 15})\n",
      "('science', 'NOUN_CHUNK', 24.0, 0, {'pos': 8, 'neg': 5, 'neu': 11})\n",
      "('@brett harris', 'PERSON', 22.0, 0, {'pos': 5, 'neg': 3, 'neu': 14})\n"
     ]
    }
   ],
   "source": [
    "# Sort\n",
    "def topic_score(n, all_likes):\n",
    "    return n + int(all_likes)*0.1\n",
    "\n",
    "def sort(named_entities_sorted, noun_chunks_sorted, n_topics=200):\n",
    "    all_ents = named_entities_sorted[:n_topics] + noun_chunks_sorted[:n_topics]\n",
    "    all_parsed = []\n",
    "    for (token, label, n, edges) in all_ents:\n",
    "        commentIds = []\n",
    "        all_likes = 0\n",
    "        all_sentiment = {'pos': 0, 'neg': 0, 'neu': 0}\n",
    "        for (commentId, likes, sentiment) in edges:\n",
    "            commentIds.append(commentId)\n",
    "            all_likes += likes\n",
    "            if sentiment > 0: \n",
    "                all_sentiment['pos'] += 1\n",
    "            elif sentiment < 0:\n",
    "                all_sentiment['neg'] += 1\n",
    "            else:\n",
    "                all_sentiment['neu'] += 1\n",
    "            score = topic_score(n, all_likes)\n",
    "        all_parsed.append((token, label, score, all_likes, all_sentiment, commentIds))\n",
    "    all_sorted = sorted(all_parsed, key=lambda e: e[2], reverse=True)\n",
    "    return all_sorted[:n_topics]\n",
    "\n",
    "all_sorted = sort(named_entities_sorted, noun_chunks_sorted)\n",
    "for s in all_sorted[:10]:\n",
    "    print(s[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Timer():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.last_time = time.time()\n",
    "        \n",
    "    def step(self, message):\n",
    "        this_time = time.time()\n",
    "        print(f'{message}: {this_time - self.last_time}')\n",
    "        self.last_time = this_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open file: 0.021181821823120117\n",
      "analyze: 22.96806311607361\n",
      "cluster: 0.026554107666015625\n",
      "named entities: 0.004396677017211914\n",
      "noun chunks: 0.1861402988433838\n",
      "sort: 0.004126787185668945\n",
      "('elon', 'NOUN_CHUNK', 193.0, 0, {'pos': 65, 'neg': 16, 'neu': 112})\n",
      "('tesla', 'NOUN_CHUNK', 189.0, 0, {'pos': 52, 'neg': 24, 'neu': 113})\n",
      "('rob', 'PERSON', 161.0, 0, {'pos': 70, 'neg': 6, 'neu': 85})\n",
      "('gme', 'ORG', 124.0, 0, {'pos': 15, 'neg': 16, 'neu': 93})\n",
      "('tsla', 'NOUN_CHUNK', 91.0, 0, {'pos': 15, 'neg': 14, 'neu': 62})\n",
      "('fsd', 'ORG', 87.0, 0, {'pos': 26, 'neg': 12, 'neu': 49})\n",
      "('stock', 'NOUN_CHUNK', 82.0, 0, {'pos': 15, 'neg': 6, 'neu': 61})\n",
      "('amc', 'ORG', 69.0, 0, {'pos': 7, 'neg': 13, 'neu': 49})\n",
      "('audio', 'NOUN_CHUNK', 60.0, 0, {'pos': 36, 'neg': 10, 'neu': 14})\n",
      "('wsb', 'NOUN_CHUNK', 54.0, 0, {'pos': 4, 'neg': 19, 'neu': 31})\n"
     ]
    }
   ],
   "source": [
    "timer = Timer()\n",
    "\n",
    "with open('data/tsla_q420_earnings.json', 'r') as f:\n",
    "    corpus = json.load(f)\n",
    "timer.step(\"open file\")\n",
    "\n",
    "for c in corpus:\n",
    "    c['id'] = hash(c['message'], c['message'], c['message'])\n",
    "    c['likes'] = 0\n",
    "\n",
    "    topics, sentiment = analyze(c['message'])\n",
    "    c['topics'] = topics\n",
    "    c['sentiment'] = round(sentiment, 3)\n",
    "timer.step(\"analyze\")\n",
    "    \n",
    "named_entities, noun_chunks, frequency = cluster_comments(corpus)\n",
    "timer.step(\"cluster\")\n",
    "\n",
    "user_subs = {'eric': 'eric weinstein', 'max': 'max tegmark', 'gu': 'geometric unity'}\n",
    "named_entities_sorted, ne_subs = cluster_ne(named_entities, frequency, user_subs)\n",
    "timer.step(\"named entities\")\n",
    "\n",
    "ne_tokens = [token for (token, label, n, edges) in named_entities_sorted]\n",
    "noun_chunks_sorted, ngram_subs = cluster_nc(noun_chunks, ne_tokens, frequency, ne_subs)\n",
    "timer.step(\"noun chunks\")\n",
    "\n",
    "all_sorted = sort(named_entities_sorted, noun_chunks_sorted)\n",
    "timer.step(\"sort\")\n",
    "\n",
    "for s in all_sorted[:10]:\n",
    "    print(s[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "batching",
   "language": "python",
   "name": "batching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
